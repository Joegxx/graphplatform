#==========================================
# tomcat
#==========================================
server.port=10051
server.context-path=/tag/api

#==========================================
# graph-tag-analytics
#==========================================
tag.analytics.task.inc.cron=0 30 0 * * ?
#tag.analytics.task.inc.cron=0 0/1 * * * ?
# default=false
tag.analytics.task.inc.enabled=false
tag.analytics.domains=crm_dev
tag.analytics.scheduler.queue.max.size=5
# TimeUnit.HOURS
tag.analytics.scheduler.batch.timeout=5
tag.analytics.schema.main=Company
# default=day
tag.analytics.partition.field=day
# default=/user/   must end with /
tag.analytics.hdfs.path=/user/

#==========================================
# graph-engine-flow
#==========================================
engine.flow.spark.driver.memory=2g
engine.flow.spark.executor.cores=4
engine.flow.spark.executor.memory=6g
engine.flow.spark.cores.max=40

#==========================================
# hive
#==========================================
hive.url=jdbc:hive2://hadoop01.sz.haizhi.com:10000/default
hive.username=root
hive.password=

#==========================================
# elasticsearch
#==========================================
es.cluster.name=graph
es.cluster.url=192.168.1.49,192.168.1.51,192.168.1.52:9300

#==========================================
# kafka
#==========================================
spring.kafka.bootstrap-servers=192.168.1.120:21007,192.168.1.121:21007,192.168.1.122:21007
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.retries=3
spring.kafka.producer.acks=all
spring.kafka.properties.max.request.size=10485760
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.group-id=tag-analytics
spring.kafka.consumer.auto-offset-reset=earliest
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.listener.ack-mode=manual
spring.kafka.consumer.max-poll-records=100000
tag.analytics.kafka.topic=tag-analytics-test

## FI Config start
spring.kafka.properties.security.protocol = SASL_PLAINTEXT
spring.kafka.properties.kerberos.domain.name = hadoop.hadoop.com
spring.kafka.properties.sasl.kerberos.service.name = kafka

# Hadoop security mode enabled must be true
graph.hadoop.security.enabled = @graph.hadoop.security.enabled@
graph.hadoop.security.user-principal = graph
### FI Config end

#==========================================
# mysql
#==========================================
spring.datasource.url=jdbc:mysql://192.168.1.59:3306/graph_qd_dev?autoReconnection=true&useSSL=true&useUnicode=true&characterEncoding=UTF-8
spring.datasource.username=crmdev
spring.datasource.password=Crm@dev_2018
spring.datasource.driverClassName=com.mysql.jdbc.Driver
spring.datasource.max-idle=10
spring.datasource.max-wait=10000
spring.datasource.min-idle=10
spring.datasource.initial-size=3

spring.jpa.generate-ddl=false
spring.jpa.show-sql=false
spring.jpa.hibernate.ddl-auto=none